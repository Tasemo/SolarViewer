\chapter{Fazit}\label{chapter7}

\section{Zusammenfassung}
Mit dieser Arbeit wurden die Problem der Datenmenge bei der Visualisierung von Geländedaten an Hand eines Prototypen aufgezeigt. Es wurde eine vollständige, interaktive 3D Visualisierung entwickelt, welche alle definierten Anforderungen erfüllt. Insbesondere die Anforderungen an den Speicherverbrauch wurden dabei eingehalten. Auch die Anforderung an die Skalierbarkeit wurde eingehalten. Durch die Integration eines weiteren Datensatzes (Höhendaten des Merkurs) in der Schlussphase dieses Projekts wurde gezeigt, dass die Architektur auch für weitere Datensätze ausgelegt ist. Es wurden verschiedene Methoden zur Datenreduktion recherchiert und alle gefundenen verlustfreien Methoden auch implementiert. Insbesondere wurde dabei die hohe Komplexität der Redundanzentfernung erläutert und anschließend ein machbarer greedy Algorithmus entwickelt. Auch einige alternative Techniken wurden recherchiert, entsprachen aber nicht der Zielstellung dieses Projekts. Als verlustbehaftete Methode wurde eine einfache systematische Reduktion realisiert. Es wurde eine Evaluation ähnlicher Visualisierungen vorgenommen und die Anforderungen an ihre Stärken und Schwächen angepasst. Anschließend wurden verschiedene Zielsysteme und Programmiersprachen gegeneinander evaluiert, wobei das Web als Zielsystem ausgewählt wurde. Anschließend wurden die während der Entwicklung aufgetretenen Probleme beschrieben, sodass dies als Ausgangspunkt für zukünftige Projekte dienen kann. Insbesondere wurden auch zwei Kameraimplementationen entwickelt und gegeneinander evaluiert, wobei die stationäre Kamera dort von allen Testern bevorzugt wurde. Die Wahl der genutzten Frameworks konnte rückblickend nicht bemängelt werden, sodass diese auch bei zukünftigen Projekten zum Einsatz kommen könnten. Die User Experience war ansonsten auf einem akzeptablem Niveau, wobei einige gewünschten Verbesserungen als sehr nützlich eingestuft wurden. Insbesondere die Hervorhebung des Höhen- und Koordinatenfeatures muss dabei verbessert werden. Die gewählte topographische Visualisierung wurde von den Testern verstanden und wird ebenfalls als positiv eingestuft.

\section{Limitationen}
Durch eine umfassende Evaluation wurde erkannt, dass der Prototyp durch das Netzwerk limitiert ist. Im Speziellen ist hier die Limitierung auf nur sechs parallele Netzwerkverbindungen durch den Browser als Problem ermittelt worden, was den Durchsatz natürlich stark einschränkt und insbesondere den Server nicht ausgelastet. Die Performance auf Client-Seite ist ansonsten auf einem akzeptablem Niveau, sodass eine Verbesserung des Netzwerkproblems hier sehr vorteilhaft wäre. Eine weitere Limitierung ist die Annahme, dass der Mars eine perfekte Kugel darstellt. Wie beschrieben wäre es korrekter, falls die Höhenwerte als Abstand vom Aeroiden darstellen würden, was zu einem realitätsnäheren Modell geführt hätte. Hier ist die Einbindung eines weiteren Datensatzes definitiv umsetzbar. Weitere Abstriche musste bei der Genauigkeit des \textit{frustum-} und \textit{occlusion culling} gemacht werden. Hier mussten sehr grob definierte Hüllobjekte verwendet werden und insbesonders beim \textit{occlusion culling} durch die ineffizienten Raycasts Kompromisse in Kauf genommen werden. Dies ist in der Anwendung bei bestimmten Kamerasichten dadurch zu erkennen, dass bestimmte Abschnitte an den Polen nicht ordnungsgemäß laden. Schlussendlich ist die relativ starke Drosselung des Ladeprozesses und die fehlende Anpassung der Performance an die Benutzer-Hardware ein Punkt, den man im Projekt verbessern könnte.

\section{Schlussfolgerungen}
Eine wichtige Schlussfolgerung dieser Arbeit ist, dass die Datenmenge keine unüberwindbare Hürde darstellt. Die Reduktion der Datenmenge durch verlustfreie Methoden sollte immer durchgeführt werden, hier konnten keine Nachteile erkannt werden. Dazu zählt die Durchführung des \textit{frustum culling} und des \textit{occlusion cullings}, aber auch die Nutzung von indizierten Modellen in OpenGL und die Bereinigung des Datensatzes von Redundanzen. Daraus ergibt sich allerdings die Notwendigkeit der Unterteilung der Welt in Abschnitte, was zu einigen Problemen bei der Entwicklung geführt hat. Als verlustbehaftete Methode bei natürlichen Höhendaten wurde die systematische Reduktion als erstaunlich gut eingestuft. Die geographischen Features konnten gut erkannt werden und es wurden keine unnatürlich Höhenunterschiede im Modell wahrgenommen. Allerdings hat sich durch eine Nutzerevaluation herausgestellt, dass eine echte 3D Visualisierung von den Testern als wenig vorteilhaft eingestuft wurde. Den Testern fielen die unterschiedlichen Höhen im Modell auf Grund der Gesamtgröße des Planeten teilweise nicht auf. Hier könnte zum einen eine künstliche Hervorhebung der Höhen implementiert werden, auch wenn dies den Realitätsgrad verringert. Auch könnte eine Visualisierung an Hand von Texturen eine Alternative darstellen, da so zwar keine echten Höhenunterschiede mehr existieren, das Problem der Datenmenge damit natürlich nahezu entfällt.

Eine andere Schlussfolgerung betrifft das Web als gewähltes Zielsystem. Gerade auf Grund der Datenmenge und der Kernentscheidung, die Quelldaten nicht im Arbeitsspeicher zu behalten, war dies eine diskussionswürdige Entscheidung. Das Web besitzt natürlich sehr viele Vorteile, unter anderem die gute Verfügbarkeit und die leichte Entwicklung von User Interfaces, allerdings ist der begrenzte Netzwerk-Durchsatz, selbst bei möglichen Verbesserungen, ein nicht überwindbarer Punkt. Die Entwicklung einer nativen Anwendung hätte ohne weitere Änderungen zu einer deutlich besseren Performance geführt. Das Profiling hat deutlich gezeigt, dass sowohl auf CPU und GPU Seite, als auch der Speicherverbrauch, bei weitem nicht ausgelastet sind. Die Designentscheidung, die Daten außerhalb des Arbeitsspeichers zu halten, wird immer noch als gut angesehen, da die Skalierbarkeit auf größere Datenmengen so gegeben ist.

\section{Ausblick}
Die Anwendung hat zwar Techniken und Problemlösungen für das Problem der Datenmenge präsentiert, allerdings kann dies noch deutlich verbessert werden. Zum einen konnte nicht gezeigt werden, dass die bestehende Architektur wirklich für noch deutlich größere Datenmengen ausgelegt ist. Ein Versuch in der Schlussphase dieses Projekts Daten der LOLA Mission mit der vierfachen Datenmenge zu integrieren um dies zu beweisen, scheiterte an der fehlenden Unterstützung des BigTIFF Datenformats (TIFF Bilder mit einer Größe von über 4 GB) durch die Java ImageIO-API. Dies ist der nächste Schritt in der Entwicklung des Projekts. Auch die Performance muss deutlich verbessert werden. Insbesondere die Implementierung der HTTP/2 Server Push Funktionalität sollte für das Projekt bedacht werden. Ansonsten gibt es weiteren Forschungsbedarf im Gebiet der verlustbehafteten Datenreduktion. Bei diesem Projekt wurde ein Ansatz gewählt, der zwar gute Ergebnisse erzielt hat, aber die Datenmenge nicht auf optimale Weise reduziert. Es besteht die Annahme, dass es Flächen innerhalb der Daten gibt, die so ähnlich sind, dass eine gezieltere Reduktion hier Vorteile erbringen würde. Hier könnten in zukünftigen Projekten komplexere Algorithmen gegeneinander evaluiert werden.