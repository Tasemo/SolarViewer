\chapter{Implementierung}

\section{Build System}
Der Build Prozess ist ein Prozess, welcher alle Schritte beinhaltet um aus dem vorhandenen Source Code eine ausführbare Software zu erstellen. Insbesondere werden dabei benötigte Abhängigkeiten zur Verfügung gestellt und in die Software integriert, der Source Code kompiliert und alle Software Teile in einem ausführbaren Format zusammengeführt. Dieser Prozess kann sehr komplex werden und ist daher häufig fehleranfällig. Unter anderem die Kombination von mehreren Build Systemen und unterschiedlichen Programmierumgebungen wird dabei als ein Kernproblem angesehen. %[GUCK DIR DAS PAPER AN: https://ieeexplore.ieee.org/abstract/document/6405265] vs https://pi.informatik.uni-siegen.de/gi/stt/38_2/01_Fachgruppenberichte/WSRE_2018/08_WSRE2018_paper_26.pdf

Da sich dieses Projekt in ein Frontend und Backend mit unterschiedlichen Programmiersprachen gliedert, ist es allerdings notwendig unterschiedliche Build Systeme zu integrieren. Dies ist erforderlich, da die existierenden Systeme sich eher auf eine Umgebung spezialisieren und zum Beispiel nur Abhängigkeiten einer Programmiersprache in einem zentralen Repository anbieten (z.B. npm registry für JavaScript oder maven central für Java Anwendungen). Auch existieren bestimmte Plugins, zum Beispiel für das Transpilieren von TypeScript, nicht für alle Build Systeme. Alles in allem erhöht sich zwar die Build Komplexität und Dauer, allerdings hat es auch Vorteile. Zum einen führt es dazu, dass das Frontend vom Backend vollständig isoliert ist und die reine Frontendentwicklung ohne unnötige Abhängigkeiten und Build Prozesse ablaufen kann. Zum anderen erweitert sich dadurch auch die Auswahl an vorhanden Tools und Plugins, was die Entwicklung deutlich vereinfachen kann.

Für dieses Projekt habe ich mich für Maven als System für das Backend und npm als System für das Frontend entschieden. Die Entscheidung wurde auf Grund der hohen Beliebtheit und der persönlichen Expertise mit diesen Tools getroffen. Aus reiner Systemsicht ist das Frontend ein Teil des Backends, da es für die Auslieferung des Frontends an den Nutzer verantwortlich ist. Der Build beginnt daher auch in Maven. Als erstes muss sichergestellt werden, dass npm und Node.js auf dem Rechner vorhanden sind. Für die Kommunikation mit diesen nutze ich das frontend-maven-plugin\footnote{\url{https://github.com/eirslett/frontend-maven-plugin}}, welches sie in einem ersten Schritt in einen lokalen Ordner installiert, solange sie noch nicht vorhanden sind. Anschließend wird “npm install” aufgerufen, um alle deklarierten Abhängigkeiten aus dem Frontend zu installieren. Dazu zählt zum Beispiel THREE.js oder auch der TypeScript Transpiler. Anschließend wird der Build an das Frontend übergeben, indem ein fest definiertes npm Script aufgerufen wird. Dieses führt als erstes den TypeScript Transpiler aus um die benötigten JavaScript Dateien anzulegen. 

% maven als Buildsystem für das Backend
% npm als Buildsystem für das Frontend
% Erkläre, wie die beiden vernetzt sind (mit Hilfe von Plugin https://github.com/eirslett/frontend-maven-plugin wird npm und node auf dem Rechner installiert, falls noch nicht vorhanden, dann wird npm install aufgerufen um alle benötigten Abhängigkeiten (TypeScript) zu installieren und abschließend wird ein “build” Script aufgerufen, welches in der package.json (npm) definiert wurde
% jetzt besteht noch das Problem mit den node-modules, dafür muss ein Bunder genutzt werden, welcher alle Abhängigkeiten in ein lokal referenzierbare Datei packt, ich hab mich für Browserify entschieden -> nope, das funktioniert nicht mit ES Modulen und erfordert weitere Abhängigkeiten -> Webpack
% Riesenvorteil: Code wird minifiziert und in einer einzigen Datei ausgelifert, schnelleres Laden der Webseite
% Riesenvorteil: das ganze kann ohne Probleme auch als Node.JS App deployed werden (naja, three.js macht da Probelem da es für das Web als Zielplatform entwicklet wird)
% Aufpassen, dass nicht unnötig Dateien im classpath landen, zum Beispiel alle config, die originalen TypeScript und auf keinen Fall der node_modules Ordner, daher eigenen Ordner für alle Entwicklunsgdateien und den Build so anpassen, dass nach dem Durchführen des TypeScript Compilers und des Webpack Bundlern die richtigen Dateien in einen Ordner auf dem classpath kopiert werden
% damit es trotzdem gedebugged werden kann, werden fürs erste sogennate source maps von webpack generiert, damit kann der minifizierte code in den Entwicklertools normal angesehen werden

\section{Visualisierung}
Für die Visualisierung wurde eine schematische Darstellung gewählt, bei der keine weiteren Datenquellen notwendig sind. Bei dieser topographische Darstellung, bekannt aus Atlanten, ist der Farbwert abhängig vom Höhenwert und wird aus vordefinierten Farbbereichen interpoliert. Der Hauptgrund für diese Wahl ist vor allem der begrenzte Projektzeitraum. Ein Einbinden von weiteren Quellen zur Bestimmung der Oberflächenbeschaffenheit (z.B. Gesteinsart oder Eis) hätte die Komplexität noch weiter erhöht und wurde in diesem Projekt als nicht durchführbar eingestuft.

Für die eigentliche Visualisierung kommen Shader zum Einsatz. Dafür muss dem Shader natürlich der Höhenwert bekannt sein. Bei einer flachen Projektion lässt sich dieser aus dem Vertex ablesen (entsprechend skalierte y-Koordinate), dies ändert sich natürlich, sobald eine sphärische Projektion zum Einsatz kommt. Die einfachste Möglichkeit ist es, dem Shader den Höhenwert in Metern neben dem Vertex als weiteres Attribut zu übergeben. Da dies die theoretische maximale Speichernutzung allerdings um weitere 4 GB erhöhen würde, was einer Erhöhung um 1/3 der Gesamtmenge entspricht, wurde dieser Ansatz verworfen. Stattdessen ist effizienter, den Höhenwert aus dem Vertex zu berechnen. Da der Höhenwert immer der Abweichung von einem vordefinierten Radius entspricht (ähnlich dem Meeresspiegel auf der Erde), muss dieser Radius einfach von der Länge des Vertex abgezogen werden. Dies funktioniert allerdings nur, wenn der Mittelpunkt des Modells auch mit dem Koordinaten-Nullpunkt übereinstimmt, da nur dann die Länge des Vertex der Distanz zum Punkt auf der Oberfläche entspricht. Um dies im Vertexshader zu implementieren, müssen ihm die verwendete Projektion, der Radius und die Skalierung von GL Einheiten zu Metern bekannt sein. Diese werden als Uniform Werte übergeben, sind also nicht abhängig von der Vertex-Anzahl und spielen daher für die maximale Speichernutzung keine Rolle. Der Vertexshader berechnet also wie beschrieben die Höhe in Metern und gibt sie an den Fragmentshader weiter. Des Weiteren transformiert er wie üblich den Vertex mit der Model-Matrix (enthält die Transformationen des Modells), der View-Matrix (enthält die inversen Transformationen der Kamera) und der Projektions-Matrix (enthält die perspektivische Transformation der Kamera) um die endgültige Position des Vertex zu bestimmen.

Der Fragmentshader hat nun die Aufgabe, aus diesem Höhenwert einen Farbwert zu generieren. Eine Möglichkeit wäre es, einfach verschiedene Grenzen zu definieren und diesen Grenzen feste Farbwerte zuzuweisen. Dann kann geprüft werden, welchem Bereich der Höhenwert entspricht und der endgültige Farbwert entspricht dann einer Variation des Farbwerts des Bereichs. Da dies allerdings mehrere Verzweigungen (conditionals) zur Prüfung der Grenzen erfordert und dies in der Shaderentwicklung vermieden werden sollte\footnote{siehe \cite{shaderDev}, Kapitel 14, Abschnitt Avoid Dynamic Branching, S. 273}, wurde eine bessere Lösung gesucht. Insbesondere, da es sich um das sogenannte dynamic branching handelt, da da die Bedingung abhängig vom Höhenwert ist, welcher natürlich pro Vertex anders ist. Des Weiteren wurde auf Grund der Datenmenge die kritischste Stelle der Performance (bottleneck) eher auf GPU Seite angesehen, sodass hier dringender auf der Performance geachtet werden sollte. Schlussendlich wurde der Fakt genutzt, das der Hue-Wert im HSV-Farbraum eine relativ lineare Verteilung verschiedener Farben enthält und so gut als Farbskala genutzt werden kann. Da die Ausgabe allerdings im RGBA-Format erfolgen muss, ist hier eine Umwandlung des HSV Werten in diesen Farbraum erforderlich. Es wird also der Prozentwert des aktuellen Höhenwerts abhängig von vom Nutzer definierten Grenzen berechnet und diesem Prozentwert ein Hue Wert zugeordnet. Dabei wurde der Farbraum vorher noch verkleinert und invertiert, sodass die Farben dann von einem Blau-Ton (niedrigster Wert) zu einem Rot-Ton (höchster Wert) reichen. 

\section{Kamerabewegung}
Die Steuerung der Kamera zur Betrachtung von verschiedenen Stellen ist ein wichtiger Aspekt im Design des Prototypen. Hierbei muss darauf geachtet werden, dass die Bewegung sowohl aus der Ferne flüssig ist, also auch bei hoher Zoomstufe noch präzise bestimmte Orte betrachtet werden können. Auch muss die Steuerung intuitiv sein und darf keine textuellen Erklärung benötigen.

Die erste Implementierung war eine frei im Raum bewegbare Kamera, welche man mit der Tastatur steuern konnte. Als konkrete Tasten wurden zum einen die Pfeiltasten als auch die übliche Alternative WASD genutzt. Diese sind allgemein als Steuerungstasten bekannt und sollten daher keiner Erklärung bedürfen. Die Kamera bewegte sich dabei entlang des lokalen Koordinatensystems der Kamera. Dieses kann dann mit der Maus entlang der x-Achse (pitch) und y-Achse (yaw) gedreht werden. Eine Drehung um die z-Achse (roll) verkompliziert die Steuerung und wurde daher nicht implementiert. Eine Bewegung der Maus auf der x-Achse führt dabei zu einer Rotation der Kamera entlang der y-Achse und eine Bewegung auf der y-Achse zu einer Rotation entlang der x-Achse. Die Blickrichtung der Kamera folgt also effektiv der Bewegung der Maus. Dabei wurde das Drehen nur beim Gedrückthalten der Maustaste (dragging) durchgeführt, da die Kamera sich sonst natürlich bei der normalen Navigation auf der Seite bewegen würde. Das Gedrückthalten ist dabei schon weniger intuitiv, allerdings entspricht es der physischen Bewegung des Ziehens an einer Seite des Globus in der realen Welt. Hier ist allerdings eine genauere Evaluation der Steuerung notwendig um eine gute User-Experience zu gewährleisten.

Wichtig bei der Implementierung ist, dass die Geschwindigkeit der Bewegung nicht von der Geschwindigkeit des Browser abhängen darf. Daher müssen alle Vektoren, welche eine Bewegung darstellen, mit der Zeit multipliziert werden, die seit dem letzten Aufruf der Bewegung vergangen ist. Nur so wird in einem bestimmten Zeitabschnitt immer die gleiche Länge zurückgelegt. Ein weiterer Aspekt ist, dass das Gedrückthalten einer Taste natürlich zu einer kontinuierlichen Bewegung führen soll. Um dies zu erkennen kann zum einen das keydown-Event des Browser genutzt werden, welches auch gesendet wird, solange die Taste gedrückt bleibt. Allerdings ist dabei eine spürbare Verzögerung zwischen erstem und nachfolgenden Events vorhanden, sodass die Bewegung initial ziemlich ruckartig erfolgt. Auch ist die Geschwindigkeit, mit der die Events gesendet werden, nicht definiert und kann so zu sehr ruckartigen Bewegungen führen, sollte die Rate weit unter der Bildwiederholrate des Monitors liegen. 

% ein weiterer Aspekt hierbei ist, dass das Gedrückhalten einer Taste natürlich nur kontinuierlichen Bewegungen führen soll, zum einen kann hierbei einfach das keydown-Event genutzt werden, da dies auch gesendet wird, wenn die Taste gedrückt bleibt
% allerdings gibt es eine spürbare Verzögerung der Events zwischen erstem Drücken und darauffolgenden Drück-Events, sodass dieser Ansatz verworfen wurde -> stattdessen

% wichtig: die Bewegung muss unabhängig von der Geschwindigkeit des Browsers sein, wenn die Geschwingkeit zum Beispiel 10 Einheiten pro Sekunde sein soll, dann müssen die Vektoren, welche die Unterschiede in der Bewegung darstellen, mit der bisher verbrauchten Zeit pro Sekunde multipliziert werden
% fürst Erste eine simple Steuerung über WASD, welche sich entlang der lokalen Kameraausrichtung bewegt
% alternativ: Kamera die sich um fixen Punkt dreht
%% erste Implementierung: Kamera zum Punkt bwegen, rotieren und dann im lokalen Koordinatensystem den ursprünglichen Bewegungsvektor zurück rechnen
%% bessere Implementierung: den Scene-Graph von THREE.js nutzen und einfach die Kamera als Kind eines Objektes definieren, das am gewünschten Punkt ist -> dann einfach das Elternobjekt normal rotieren

% jedes Nutzervenet, welcher zu einer Änderung der Kamerasicht fürht, führt zu einem Versenden eines Events, welche den Lade Prozess[REFERENZ] in Gang setzt
% throttling erklären, keyboard und mouse events hohe Frequenz (gibt es dazu eine offizielle Aussage?), daher lieber maximal ein Event pro Sekunde um den Lade- und Endladevorgang nicht zu überfordern

\section{•}
