\chapter{Grundlagen}

\section{Datenquellen}
Die Datengrundlage für dieses Projekt sind Daten des Mars Orbiter Laser Altimeter (MOLA), einem Höhenmessgerät an Bord der Mars Global Surveyor (MGS) Raumsonde. Diese befand sich ab Ende 1997 über drei Jahre im Mars Orbit und kartographierte in dieser Zeit die Oberfläche, womit vor allem geologische Rückschlüsse über die Entstehung und die Zusammensetzung des Mars getroffen werden konnten. In dieser Zeit wurden insgesamt über 600 Millionen Messwerte gesammelt, welche dann zur Bestimmung der eigentlichen Höhe genutzt wurden. Die Höhe eines Planeten wird immer in Abweichung von einer Referenzhöhe angegeben (“Meeresspiegel”) und wird auch als orthometrische Höhe bezeichnet. Ursprünglich wurde zur Bestimmung der Referenzhöhe des Mars ein Atmosphärendruck von 6.1 mbar genutzt, was dem durchschnittlichen Druck auf dem Mars entspricht. Da dies jedoch abhängig von saisonalen Schwankungen ist, wurde eine Definition an Hand der Schwerkraft entwickelt. Die Fläche um den Planeten bei denen alle Punkte dasselbe energetische Potential besitzen (abhängig von der der Schwerkraft und der Zentrifugalkraft), wird ähnlich dem Geoiden auf der Erde Areoid genannt. Wichtig ist, dass dieses Modell auf Grund von Dichteunterschieden keine sphärische Form besitzt und so für eine akkurate 3D Darstellung auch die Form des Areoiden für jeden Höhenwert bekannt sein müsste. Da dafür jedoch ein weiterer Datensatz notwendig wäre, wurde für dieses Projekt ein einfacheres Modell genutzt, bei dem die Werte als Abstand von dem durchschnittlichen Radius des Areoid angesehen werden. Es wird in diesem Modell also davon ausgegangen, dass der Mars eine perfekte Kugel darstellt. Die horizontale Genauigkeit, also die maximale Abweichung vom originalen Messort, der Werte liegt bei ungefähr 100 m und die vertikale Genauigkeit, also die maximale Abweichung vom eigentlichen Höhenwert, bei ungefähr 3 m. 

Die Daten sind frei verfügbar und werden von Planetary Data System (PDS) bereitgestellt\ref{molaData}. Die originalen Daten werden dabei in verschiedenen Auflösungen zur Verfügung gestellt, wobei sich dieses Projekt an der höchsten Auflösung versucht. Dabei existieren pro Breiten- und Längengrad 128 Werte, was einer Auflösung von ungefähr 463 m entspricht. Die Daten werden bereits in 16 verschiedenen Dateien bereitgestellt, welche die Höhenwerte als jeweils 16 Bit Wert nacheinander speichert. Des Weiteren stellen sie auch die Werte des Areoiden bereit. Zusätzlich zum sehr einfachen Datenformat bietet sich diese Datenquelle natürlich an, allerdings wurden ursprünglich nur Werte zwischen den Breitengraden -88 ° bis 88 ° erfasst. Da dies die Erstellung eines vollständigen Modells natürlich erschwert, soll ein modifizierter Datensatz für dieses Projekt genutzt werden, welcher bis zum Jahr 2020 verbessert wurde\ref{molaDataExtended}. Dieser wird allerdings nur in der Form eines TIFF Bildes zur Verfügung gestellt, bei dem jeder Pixel ein Messwert darstellt. Das Format von TIFF ist nicht trivial lesbar, sodass hier ein entsprechendes Tool notwendig sein wird, um die Daten auslesen zu können. Auch der Areoid ist hier nicht mit enthalten.

\section{OpenGL}
% nur für dieses Projekt relevante Aspekte von OpenGL
% Grundlagen: nur Interface, verschiedene Implementationen in Form von Driver
% existiert auch WebGL ← Unterschiede und Gemeinsamkeiten, z.B. 
% Erklärung Shader Pipeline
% Shadersprache GLSL
% Erklärung Kommunikation mit Shader (Attribut vs Uniform)
% Koodinaten Systeme -> wie verlaufen die Achsen (positive x-Achse nach rechts, positive y-Achse nach oben und positive z-Achse in den Bildschirm hinein)

\section{Datenreduzierung}\label{datenreduzierung}

\subsection{Datenmenge}\label{datenmenge}
Der beschriebene MOLA-Datensatz besitzt eine Breite von 46080 Pixeln und eine Höhe von 23040 Pixeln. Jeder Pixel ist dabei 16 Bit groß, sodass die Rohdaten an sich eine Größe von 1,98 GB besitzen\cite{molaDataExtended}. Um daraus ein 3D-Modell zu erstellen, müssen neben den Höhenwerten natürlich auch die x- und z-Position erfasst werden, welche sich aus dem Rasterformat der Daten errechnen lassen. Erschwerend hinzu kommt, dass der kleinste numerische Datentyp in GLSL 32 Bit groß ist\cite[Abschnitt 4.1, S. 23]{glslSpec} und sich die Höhendaten so nicht effizient speichern lassen. Eine entsprechende Erweiterung (\textit{extension}) von GLSL um 16 Bit Typen mit dem Namen EXT\_shader\_16bit\_storage existiert, befindet sich allerdings erst in der Entwurfsphase und kann daher nicht genutzt werden. Die reinen Vertex-Daten besitzen also eine Größe von 11,88 GB. Weitere Daten pro Vertex (zum Beispiel Normalenvektoren) werden für dieses Projekt nicht benötigt. Allerdings können diese Vertex-Daten in dieser Form nicht an OpenGL übergeben werden, da erst Polygone (OpenGL Primitives) daraus erstellt werden müssen. Im einfachsten Fall sind dies für ein Rastermodell natürlich Vierecke (GL\_QUAD), diese Form ist allerdings veraltet und sollten nicht mehr benutzt werden. Stattdessen müssen die Vertex-Daten  Dreiecke (GL\_TRIANGLE) beschreiben. Da in einem Rastermodell aus Dreiecken natürlich ein Vertex 6 mal wiederholt werden müsste, gibt es eine effizientere Beschreibung der Polygone: Indexierung. Dabei wird ein Array an OpenGL übergeben, welche die Reihenfolge der Vertices als deren Position im ursprünglichen Vertex-Array kodiert (siehe Abbildung \ref{indexing}). 

\begin{figure}[H]
  \includegraphics{indexing.png}
  \caption{Indexierung eines Raster-Modells}
  \label{indexing}
\end{figure}

Durch diese Technik wird das Wiederholen eines Vertex verhindert, allerdings ist auch hier ein Speicherverbrauch zu berechnen. Der Index entspricht dabei einer Größe von 32 Bit und die Anzahl lässt sich mit folgender Formel berechnen: \[Anzahl = 6 * (H\ddot{o}he - 1) * (Breite - 1) = 6.369.684.486\]. Dadurch werden weitere 23,73 GB benötigt, wodurch der Gesamtspeicherverbrauch auf 35,61 GB steigt. Da dies, auch ohne nicht-funktionale Anforderungen definiert zu haben, nicht in einen durchschnittlichen Grafikspeicher passt, sind Maßnahmen zur Reduzierung der Daten zwingend erforderlich.

\subsection{Verlustfrei}
Das Durchführen von verlustfreien Methoden zur Reduzierung der Datenmenge ist natürlich immer den verlustbehafteten Methoden vorzuziehen, da ...
Die einfachste Möglichkeit, ist die Entfernung von Redundanzen. Diese können auf Grund des Rasterformats der Daten vorhanden sein, da die Abstände zwischen den Datenpixeln laut Definition einem festen Wert entsprechen müssen. Die hohe horizontale Datenauflösung von 463 m bei einer natürlichen Topographie führt zu der statistischen Annahme, dass dabei Redundanzen auftreten. Hier stellt sich die Frage, was eigentlich eine Redundanz im Kontext von 2D Höhenwerten ist. Ein Auffinden von gleichen Werten in einem 3x3 Raster ist der einfachste Fall. Wichtig hierbei ist, dass nicht etwa ein einfaches Wiederholen von Werten bereits eine Redundanz ist, da auch das Darstellen einer flachen Ebene eine wichtige Information ist. Allerdings können Redundanzen auch bei einer geneigten Ebene auftreten, bei der sich benachbarte Höhenwerte unterscheiden. Der generelle Fall zur Beschreibung ist das Vorhandensein einer linearen Abhängigkeit zwischen Werten in \textbf{allen} Spalten und Reihen eines mindestens 3x3 großen Rasters\cite{topoDataReduction}. Wichtig hierbei ist, dass immer der 2D Kontext beachtet werden muss. Es können sich beliebig viele Werte in einer Reihe oder Spalte wiederholen, ohne das dies eine Redundanz darstellt, wenn auch nur eine Spalte oder Reihe im Raster diese lineare Abhängigkeit nicht aufweist. Um das Vorhandensein von Redundanzen im MOLA Datensatz zu beweisen, wurde ein Script erstellt, welches im Abschnitt \ref{redundanzberechnung} genauer beschrieben ist. Dies zeigt, dass insgesamt 68.264.527 Datenpixel redundant sind, was einem Prozentsatz von 6,43 \% entspricht. Dabei existieren 8.708.542 redundante Teilraster, das größte von ihnen besitzt immerhin eine Fläche von 277.922 km².

Eine andere Möglichkeit eine verlustfreie Reduktion zu erreichen, ist die Daten zu entfernen, die vom Benutzer nicht gesehen werden können. Das betrifft explizit nicht den Punkt, dass Unterschiede auf Grund der Physiologie des menschlichen Auges nicht wahrgenommen werden können, da dies trotzdem einem theoretischen Informationsverlust entspricht.

% frustum culling, also das Entfernen von Daten außerhalb des Sichtbereichs der Kamera (Frustum).
% collucion culling, also das Entfernen von Daten, die durch andere Daten verdeckt werden
% dies ist nach der Projektion auf eine Kugel der Fall, wenn die Daten am anderen Ende des Globus liegen
% Vorraussetzung: Unteilung des ganzen Modells in kleinere Abschnitte (chunks), für jeden Abschnitt muss geprüft werden, ob er durch eine der beiden Methoden ausgeschlossen werden kann
% solange auch nur ein Teil des Abschnitts sichbar ist, muss der ganze Abschnitt angezeigt werden

\subsection{Verlustbehaftet}
Das Durchführen von verlustfreien Methoden allein reicht nicht aus, um den Speicherverbrauch auf ein akzeptables Niveau zu senken.

% systematische reduktion, mit fester Schritteite (stride) über die Daten iterieren -> Vorteil: die Indexierung bleibt erhalten nur die Abstände vergößern sich
% hier ausnutzen, dass zum Beispiel bei geringen Zoomstufen Unterschiede in den Daten nicht mehr wahrgenommen werden können (physiologie)
% geringe Änderungen in einem 3D Kontekt, auch oft als geringe räumliche Frequenz beschrieben, entfernen
% Reduktion der Datenmenge abhängig vom umliegenden Terrain -> ähnliche vertices müssen nicht wiederholt werden,
% komplexere mesh simplification Verfahren (nenne Beispiele) sind nicht notwendig, da natürliche Daten einfach zu ähnlich sind 

\subsection{Redundanzentfernung}\label{redundanzberechnung}
Bevor ein geeigneter Algorithmus gefunden werden kann, müssen einige Probleme mit der Entfernung von Redundanzen besprochen werden. Zum einen besteht das Problem, die gefundenen Lücken in einem Datenformat darzustellen. Normalerweise besteht immer der gleiche Abstand zwischen zwei benachbarten Datenpunkte, wenn jetzt einfach Vertices entfernt werden, dann ändern sich die Abstände und dies muss irgendwie kodiert werden. Eine Möglichkeit dies zu umgehen, ist die Entfernung zur Laufzeit durchzuführen und direkt bei der Erstellung redundante Vertices zu erkennen. Da der hier verwendete Algorithmus allerdings sehr kostenaufwendig ist, wurde dieser Ansatz verworfen. Stattdessen werden die Lücken durch vordefinierte Werte kodiert, die nicht in den originalen Daten vorhanden sind. Im konkreten Fall wurde der kleinstmögliche 16 Bit Wert gewählt.

Zum anderen besteht das Problem der Indexierung des Modells. In einem reinen Raster-Modell lassen sich die Indices trivial berechnen, da sie immer einem gleichmäßigem Muster folgen. Jetzt existieren natürlich Lücken in den Vertex-Daten und eine neues Polygon Netz muss gefunden werden. Ein Problem dabei ist, dass benachbarte Vertices sich mit den Rändern der Lücken verbinden müssen (siehe Abbildung x). Auch wenn die Ränder also redundante Daten aufweisen, müssen sie für die Erzeugung eines korrekt aussehenden Modells zugelassen werden. Anstatt also nur die Eckpunkte zu erhalten, müssen auch alle Ränder der Redundanzen erhalten bleiben, was den Prozentsatz der redundanten Vertices auf 1,87 \% drückt. Für die Berechnung der Indices wurde folgende Formel genutzt: TODO.

Wie beschrieben, ist die Unterteilung des Gesamtmodells in Abschnitte ein zentraler Aspekt der Anwendung um das \textit{frustum-} und \textit{occlusion culling} durchführen zu können. Die führt aber zu einer weiteren Einschränkung bei der Entfernung von Redundanzen, die über Abschnittsgrenzen hinaus gehen würden. Die müssen an den Rändern der Abschnitte definiert sein, um sich mit benachbarten Abschnitten verbinden zu können. Auch sollen per Design Abschnitte keinen Zugriff auf Vertex-Daten anderer Abschnitte erhalten (schon allein um Daten nicht unnötig im RAM zu halten). Dies führt dazu, dass der Algorithmus zusätzlich redundante Teilraster an den Abschnittsgrenzen trennen muss. Dies verringert den Prozentsatz allerdings nur minimal auf 1,86 \%.

Nachdem nun Anforderungen an den Algorithmus definiert wurden, muss das Problem der hohen Komplexität angegangen werden. Die optimale Lösung ist durch die größtmögliche Entfernung von Vertices definiert, was auch durch eine möglichst große Summe des Flächeninhalts aller Teilraster beschrieben werden kann. Das Problem ist, dass während der Laufzeit nicht sichtbar ist, welche Konfiguration der Raster zur größtmöglichen Fläche führt. Es kann durchaus sein, dass in einem Teilschritt ein kleineres Raster gebildet werden muss, damit übrig gebliebene Vertices Teil eines deutlich größeren Rasters werden. Das diesem Problem am nächsten stehende bekannte Problem aus der Informatik ist das Problem des \textit{rectangle packing}, einer 2D Variation des Rucksack-Problems, welches der Komplexitätsklasse NP-Vollständig zugerechnet wird. Die Überprüfung ob eine Lösung optimal ist, erfordert dabei das Überprüfen aller anderen Lösungen, was eine exponentielle Zeit benötigt. Allerdings sind im konkreten Fall einige Einschränkungen vorhanden, die die Komplexität verringern. Zum einen können die Rechtecke nicht an jede Stelle gepackt werden, sondern nur in den Flächen, die als Redundanzen in Frage kommen. Zum anderen ist die Mindestgröße eines Rechtecks 3x3, sodass sich die Anzahl der Möglichkeiten um den Faktor 9 verringert. Unter der Annahme, dass die aktuelle, nicht optimale Berechnung die Anzahl der Gesamtredundanzen mit ungefähr 2 \% widerspiegelt, ist die Komplexität: \[O(n) = O(1.061.683.200) = 2^{n / 9 * 2 / 100} = 7.32 * 10^{710.218}\].
´
Als machbare Alternative wurde hier ein Algorithmus aus der Klasse der greedy Algorithmen entwickelt. Dabei wird in jedem Teilschritt des Algorithmus immer die aktuell beste Lösung, also das größte Teilraster, gewählt. Die gefundene Lösung ist damit zwar nicht unbedingt optimal, aber es wird ein lokales Minimum erreicht. Der größte Vorteil dabei ist, das die Lösung mit fast linearer Komplexität gefunden werden kann. Die Komplexität ist leicht erhöht, da pro Wert immer eine nicht bestimmbare Anzahl an nachfolgenden Werten überprüft werden muss. Diese können aber potentiell bei nachfolgenden Iterationen übersprungen werden, falls sie als redundant eingestuft wurden. Der Algorithmus (siehe Algorithmus \ref{redundanz_algorithmus}) iteriert über alle Werte und sucht für jeden nicht besuchten Datenpunkt alle nachfolgenden horizontalen und vertikalen linearen Abhängigkeiten. Dabei werden jeweils die Breiten und Längen dieser Abhängigkeiten zurückgeliefert und auch alle Alternativen in Betracht gezogen. Es können zum Beispiel bei den Reihen jeweils 3 Reihen eine Breitenlänge von 4 haben oder 5 Reihen eine Breitenlänge von 3. Beides kann zum größtmöglichen Raster führen. Dies wird auch für die Spalten durchgeführt und die größtmögliche Überschneidung aus Spalten und Reihen wird als redundant markiert, solange sie die Mindestgröße von 3x3 erreicht hat.

\begin{algorithm}[H]
\begin{algorithmic}
\caption{Redundanzentfernung}
\label{redundanz_algorithmus}
\Procedure{findRedundancies}{data, replacement, chunkSize}
    \ForAll{x, z \textbf{in} data}
        \If{Wert an Stelle x, z noch nicht besucht}
            \State rows = Finde alle Abhängigkeiten in den Reihen ab x, z (checkRows)
            \State columns = Finde alle Abhängigkeiten in der Spalten ab x, z
            \State raster = Finde das Raster mit dem größten Flächeninhalt in rows, columns
            \If{raster hat Mindestgröße 3x3}
                \State Ersetze alle Werte in raster, die nicht Kanten sind, mit replacement
                \State Markiere alle Werte in raster als besucht
            \EndIf
        \EndIf     
    \EndFor
\EndProcedure
\Procedure{checkRows}{data, currentX, currentZ, chunkSize}
    \ForAll{z von currentZ bis Ende der Reihen oder Ende des Chunks}
        \State difference = data[z][x] - data[z][x + 1]
        \ForAll{x von currentX bis Ende der Spalte oder Ende des Chunks}
            \If{nächste Differenz == difference}
                \State Inkrementiere aktuelle Breite
            \ElsIf{aktuelle Breite $<$ 3}
                \State\Return alle Breitenlängen und deren Reihen
            \ElsIf{aktuelle Breite $<$ letzte Breite}
                \State Speichere Breitenlänge und Reihe
                \State letzte Breite = aktuelle Breite
                \State Weiter mit nächster Reihe
            \Else
                \State Weiter mit nächster Reihe
            \EndIf
        \EndFor
    \EndFor
    \State\Return alle Breitenlängen und deren Reihen
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Alternative Verfahren}

% beschreibe hier zum Beispiel, dass Bilder anstelle von 3D Modellen verwendet werden können
% finde weitere Alternativen (2)